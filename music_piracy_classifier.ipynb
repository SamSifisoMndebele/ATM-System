{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamSifisoMndebele/ATM-System/blob/master/music_piracy_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW063Pn0r1-k"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T52nngxxrxGK",
        "outputId": "aa047033-b294-42d3-d34c-9cf859b16583",
        "collapsed": true
      },
      "source": [
        "%pip install scikit-learn pandas numpy beautifulsoup4 tldextract python-whois joblib matplotlib requests lxml playwright asyncio"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.12/dist-packages (0.9.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.54.0)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawler"
      ],
      "metadata": {
        "id": "SSK9JlKTnPI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawling scripts"
      ],
      "metadata": {
        "id": "H_atjF81nUR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "\n",
        "from playwright.async_api import async_playwright, Page, BrowserContext\n",
        "\n",
        "USER_AGENTS = [\n",
        "    # A few realistic desktop UAs. Rotate to reduce consistent fingerprinting.\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/124.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
        "    \"Version/17.4 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0\",\n",
        "]\n",
        "# Base directory for screenshots\n",
        "SCREENSHOTS_DIR = Path(__file__).parent / \"ai\" / \"data\" / \"screenshots\"\n",
        "SCREENSHOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "async def accept_consent_if_present(page: Page) -> None:\n",
        "    # Try to handle consent within an iframe (commonly served from consent.google.com)\n",
        "    try:\n",
        "        await page.wait_for_load_state(\"domcontentloaded\")\n",
        "        consent_frame = next((f for f in page.frames if \"consent\" in f.url.lower()), None)\n",
        "        targets = [\n",
        "            \"button:has-text('I agree')\",\n",
        "            \"button:has-text('Agree')\",\n",
        "            \"button:has-text('Accept all')\",\n",
        "            \"button:has-text('Accept')\",\n",
        "            '[aria-label*=\"Accept\"]',\n",
        "            '[data-testid=\"action-bar-accept\"]',\n",
        "        ]\n",
        "        if consent_frame:\n",
        "            for sel in targets:\n",
        "                try:\n",
        "                    btn = await consent_frame.wait_for_selector(sel, timeout=2500)\n",
        "                    await btn.click()\n",
        "                    return\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        for sel in targets:\n",
        "            try:\n",
        "                btn = await page.wait_for_selector(sel, timeout=1500)\n",
        "                await btn.click()\n",
        "                return\n",
        "            except Exception:\n",
        "                continue\n",
        "    except Exception:\n",
        "        pass  # best-effort only\n",
        "\n",
        "\n",
        "def build_search_url(query: str, num: int = 10, hl: str = \"en\", gl: str | None = None) -> str:\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"hl\": hl,\n",
        "        \"num\": str(num),\n",
        "        \"pws\": \"0\",  # turn off personalized results\n",
        "    }\n",
        "    if gl:\n",
        "        params[\"gl\"] = gl\n",
        "    return f\"https://www.google.com/search?{urllib.parse.urlencode(params)}\"\n",
        "\n",
        "\n",
        "def is_blocked(page: Page) -> bool:\n",
        "    # Detect \"sorry\" (bot-check) flow\n",
        "    url = page.url.lower()\n",
        "    if \"/sorry/\" in url:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def pick_user_agent(attempt: int) -> str:\n",
        "    # Rotate UA across attempts\n",
        "    idx = attempt % len(USER_AGENTS)\n",
        "    return USER_AGENTS[idx]\n",
        "\n",
        "\n",
        "async def make_context(p, attempt: int, headless: bool = True) -> BrowserContext:\n",
        "    ua = pick_user_agent(attempt)\n",
        "    browser = await p.chromium.launch(\n",
        "        headless=headless,\n",
        "        args=[\n",
        "            \"--disable-blink-features=AutomationControlled\",\n",
        "            \"--disable-dev-shm-usage\",\n",
        "            \"--no-sandbox\",\n",
        "        ],\n",
        "    )\n",
        "    context = await browser.new_context(\n",
        "        user_agent=ua,\n",
        "        viewport={\"width\": 1366, \"height\": 768},\n",
        "        locale=\"en-ZA\",\n",
        "        timezone_id=\"Africa/Johannesburg\",\n",
        "    )\n",
        "\n",
        "    # Reduce automation fingerprint: hide navigator.webdriver on Chromium\n",
        "    await context.add_init_script(\n",
        "        \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\"\n",
        "    )\n",
        "\n",
        "    await context.set_extra_http_headers({\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Cache-Control\": \"no-cache\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    })\n",
        "\n",
        "    # Block images and fonts to reduce network noise (helps speed and consistency)\n",
        "    async def route_handler(route, request):\n",
        "        try:\n",
        "            rtype = request.resource_type\n",
        "        except Exception:\n",
        "            rtype = \"\"\n",
        "        if rtype in {\"image\", \"font\"}:\n",
        "            await route.abort()\n",
        "        else:\n",
        "            await route.continue_()\n",
        "\n",
        "    await context.route(\"**/*\", route_handler)\n",
        "    return context\n",
        "\n",
        "\n",
        "def _normalize_google_result_url(url: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Unwrap Google /url?q=... redirect links and ensure http(s) target.\n",
        "    Returns the cleaned absolute URL or None if it's not a usable web target.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        return None\n",
        "\n",
        "    # Full absolute http(s) links\n",
        "    if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
        "        parsed = urllib.parse.urlparse(url)\n",
        "        if parsed.netloc and parsed.scheme in {\"http\", \"https\"}:\n",
        "            # Filter out obvious Google-owned hosts and special pages\n",
        "            host = parsed.hostname or \"\"\n",
        "            if any(host.endswith(h) for h in (\n",
        "                \"google.com\", \"google.co.za\", \"googleusercontent.com\", \"gstatic.com\", \"youtube.com\"\n",
        "            )):\n",
        "                return None\n",
        "            return url\n",
        "        return None\n",
        "\n",
        "    # Google redirect links like /url?q=...\n",
        "    if url.startswith(\"/url?\"):\n",
        "        qs = urllib.parse.parse_qs(urllib.parse.urlparse(url).query)\n",
        "        target = qs.get(\"q\", [None])[0]\n",
        "        if target and (target.startswith(\"http://\") or target.startswith(\"https://\")):\n",
        "            return _normalize_google_result_url(target)\n",
        "        return None\n",
        "\n",
        "    # Skip anchors or relative links\n",
        "    return None\n",
        "\n",
        "\n",
        "async def collect_results(page: Page, min_results: int = 10) -> list[dict[str, str]]:\n",
        "    results: list[dict[str, str]] = []\n",
        "\n",
        "    # Primary locator\n",
        "    await page.wait_for_selector('a:has(h3)', timeout=20000)\n",
        "\n",
        "    # Fallback: focus on the result container if present\n",
        "    anchors = await page.query_selector_all('#search a:has(h3)')\n",
        "    if not anchors:\n",
        "        anchors = await page.query_selector_all('a:has(h3)')\n",
        "\n",
        "    seen: set[str] = set()\n",
        "    for a in anchors:\n",
        "        try:\n",
        "            h3 = await a.query_selector('h3')\n",
        "            title = (await h3.inner_text()).strip() if h3 else (await a.inner_text()).strip()\n",
        "            raw_url = await a.get_attribute('href')\n",
        "            url = _normalize_google_result_url(raw_url or \"\")\n",
        "            if url and title and url not in seen:\n",
        "                seen.add(url)\n",
        "                results.append({\"title\": title, \"url\": url})\n",
        "                if len(results) >= min_results:\n",
        "                    break\n",
        "        except Exception:\n",
        "            continue\n",
        "    return results\n",
        "\n",
        "\n",
        "async def _go_to_next_page(page: Page) -> bool:\n",
        "    \"\"\"\n",
        "    Attempts to navigate to the next search results page.\n",
        "    Returns True if navigation was triggered and completed, otherwise False.\n",
        "    \"\"\"\n",
        "    selectors = [\n",
        "        'a[aria-label=\"Next page\"]',\n",
        "        'a#pnnext',\n",
        "        'a[aria-label=\"Next\"]',\n",
        "        'a:has-text(\"Next\")',\n",
        "    ]\n",
        "    for sel in selectors:\n",
        "        try:\n",
        "            el = await page.wait_for_selector(sel, timeout=2500)\n",
        "            # Ensure element is visible and enabled\n",
        "            await el.scroll_into_view_if_needed()\n",
        "            await el.click()\n",
        "            await page.wait_for_load_state(\"domcontentloaded\")\n",
        "            try:\n",
        "                await accept_consent_if_present(page)\n",
        "            except Exception:\n",
        "                pass\n",
        "            # Wait for results to appear on the new page\n",
        "            await page.wait_for_selector('a:has(h3)', timeout=10000)\n",
        "            return True\n",
        "        except Exception:\n",
        "            continue\n",
        "    return False\n",
        "\n",
        "def sanitize_filename(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Make a safe filename across platforms by removing/ replacing invalid characters\n",
        "    and trimming length.\n",
        "    \"\"\"\n",
        "    # Replace invalid Windows characters: \\ / : * ? \" < > | and control chars\n",
        "    invalid = '<>:\"/\\\\|?*'\n",
        "    cleaned = \"\".join((\"_\" if ch in invalid or ord(ch) < 32 else ch) for ch in name)\n",
        "    # Strip leading/trailing spaces and dots (Windows quirk)\n",
        "    cleaned = cleaned.strip(\" .\")\n",
        "    # Limit to a reasonable length\n",
        "    return cleaned[:150] if len(cleaned) > 150 else cleaned\n",
        "\n",
        "async def run_search(\n",
        "    query: str,\n",
        "    results_per_page: int = 10,\n",
        "    pages_num: int = 1,\n",
        "    headless: bool = True,\n",
        "    screenshots: bool = False,\n",
        ") -> list[dict[str, str]]:\n",
        "    retries = 3\n",
        "    backoff_base = 2.0\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        context: BrowserContext | None = None\n",
        "        page: Page | None = None\n",
        "        try:\n",
        "            safe_name = sanitize_filename(query)\n",
        "            for attempt in range(retries):\n",
        "                # Fresh context per attempt helps avoid sticky blocks\n",
        "                if context:\n",
        "                    await context.close()\n",
        "\n",
        "                context = await make_context(p, attempt, headless)\n",
        "                page = await context.new_page()\n",
        "\n",
        "                # Navigate directly to the search URL with stable params\n",
        "                url = build_search_url(query, num=results_per_page, hl=\"en\")\n",
        "                await page.goto(\"https://www.google.com\", wait_until=\"domcontentloaded\")\n",
        "                await accept_consent_if_present(page)\n",
        "\n",
        "                await page.goto(url, wait_until=\"domcontentloaded\")\n",
        "\n",
        "                # If blocked, retry with backoff and new UA\n",
        "                if is_blocked(page):\n",
        "                    # Keep a screenshot for diagnostics\n",
        "                    try:\n",
        "                        await page.screenshot(path=SCREENSHOTS_DIR / f\"blocked_attempt_{attempt+1}.png\", full_page=True)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    if attempt < retries - 1:\n",
        "                        sleep_for = backoff_base ** attempt + random.uniform(0.25, 0.75)\n",
        "                        await page.close()\n",
        "                        await context.close()\n",
        "                        await asyncio.sleep(sleep_for)\n",
        "                        continue\n",
        "                    else:\n",
        "                        raise RuntimeError(\"Blocked by Google (sorry page). Retries exhausted.\")\n",
        "\n",
        "                # Not blocked, proceed to collect results\n",
        "                all_results: list[dict[str, str]] = []\n",
        "                seen_urls: set[str] = set()\n",
        "\n",
        "                for page_index in range(pages_num):\n",
        "                    # Take a screenshot for each page debugging/verification\n",
        "                    if screenshots:\n",
        "                        try:\n",
        "                            page_screenshot_path = SCREENSHOTS_DIR / f\"{safe_name}_{page_index + 1}.png\"\n",
        "                            await page.screenshot(path=page_screenshot_path, full_page=True)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                    # Collect results from the current page\n",
        "                    page_results = await collect_results(page, min_results=results_per_page)\n",
        "                    # Deduplicate by URL to avoid repeats across pages\n",
        "                    for r in page_results:\n",
        "                        if r[\"url\"] not in seen_urls:\n",
        "                            seen_urls.add(r[\"url\"])\n",
        "                            all_results.append(r)\n",
        "\n",
        "                    # If we've reached the last page requested, stop\n",
        "                    if page_index >= pages_num - 1:\n",
        "                        break\n",
        "\n",
        "                    # Try to move to the next page, otherwise stop\n",
        "                    moved = await _go_to_next_page(page)\n",
        "                    if not moved:\n",
        "                        break\n",
        "\n",
        "                    # Optional small jitter to look more human and avoid rate-limits\n",
        "                    await asyncio.sleep(random.uniform(0.3, 0.8))\n",
        "\n",
        "                return all_results\n",
        "\n",
        "            # If the loop exits without a return, treat as failure\n",
        "            raise RuntimeError(\"Failed to retrieve results for an unknown reason.\")\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            try:\n",
        "                if page:\n",
        "                    await page.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                if context:\n",
        "                    await context.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "async def search(\n",
        "    query: str,\n",
        "    min_results: int = 50,\n",
        "    results_per_page: int = 20,\n",
        "    headless: bool = True,\n",
        "    screenshots: bool = False,\n",
        ") -> list[dict[str, str]]:\n",
        "    pages = math.ceil(float(min_results) / results_per_page)\n",
        "    results = await run_search(\n",
        "        query=query,\n",
        "        results_per_page=results_per_page,\n",
        "        pages_num=pages,\n",
        "        headless=headless,\n",
        "        screenshots=screenshots\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "def _hostname(url: str) -> str:\n",
        "    try:\n",
        "        return urllib.parse.urlparse(url).hostname or \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def _load_existing_csv(csv_path: Path) -> tuple[list[dict[str, str]], set[str], set[str]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - rows (existing rows as dicts with keys: url,label)\n",
        "      - existing_urls (set of URL strings)\n",
        "      - existing_hosts (set of hostnames)\n",
        "    \"\"\"\n",
        "    rows: list[dict[str, str]] = []\n",
        "    urls: set[str] = set()\n",
        "    hosts: set[str] = set()\n",
        "    if csv_path.exists():\n",
        "        with csv_path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                url = row.get(\"url\", \"\") or \"\"\n",
        "                label = row.get(\"label\", \"\")\n",
        "                rows.append({\"url\": url, \"label\": label})\n",
        "                if url:\n",
        "                    urls.add(url)\n",
        "                    hosts.add(_hostname(url))\n",
        "    return rows, urls, hosts\n",
        "\n",
        "\n",
        "def _write_seed_csv(csv_path: Path, rows: list[dict[str, str]]) -> None:\n",
        "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"url\", \"label\"])\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow({\"url\": r[\"url\"], \"label\": r.get(\"label\", \"\")})\n",
        "\n",
        "\n",
        "async def seed_urls_from_queries(\n",
        "    queries: list[str],\n",
        "    out_csv: str = \"../data/seed_urls.csv\",\n",
        "    min_results_per_query: int = 50,\n",
        "    results_per_page: int = 20,\n",
        "    headless: bool = True,\n",
        "    screenshots: bool = False,\n",
        "    dedupe_by_host: bool = True,\n",
        "    max_per_host: int = 2,\n",
        "    default_label: str = \"\",\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Runs Google searches for each query, aggregates result URLs, and writes/updates\n",
        "    a seed CSV with columns: url,label.\n",
        "\n",
        "    - Deduplicates against existing CSV (by URL) and can limit per-host inclusion.\n",
        "    - default_label left empty by default; you can manually label later.\n",
        "    \"\"\"\n",
        "    out_path = Path(out_csv)\n",
        "    existing_rows, existing_urls, existing_hosts = _load_existing_csv(out_path)\n",
        "\n",
        "    # Collect across queries\n",
        "    aggregated: list[dict[str, str]] = []\n",
        "    host_counts: dict[str, int] = {}\n",
        "    for q in queries:\n",
        "        try:\n",
        "            results = await search(\n",
        "                query=q,\n",
        "                min_results=min_results_per_query,\n",
        "                results_per_page=results_per_page,\n",
        "                headless=headless,\n",
        "                screenshots=screenshots,\n",
        "            )\n",
        "        except Exception:\n",
        "            results = []\n",
        "\n",
        "        for r in results:\n",
        "            url = r[\"url\"]\n",
        "            if not url or url in existing_urls:\n",
        "                continue\n",
        "            host = _hostname(url)\n",
        "            if dedupe_by_host:\n",
        "                count = host_counts.get(host, 0)\n",
        "                if count >= max_per_host:\n",
        "                    continue\n",
        "                host_counts[host] = count + 1\n",
        "            aggregated.append({\n",
        "                \"url\": url,\n",
        "                \"label\": default_label\n",
        "            })\n",
        "\n",
        "    # Merge and write back\n",
        "    merged = existing_rows + aggregated\n",
        "    # Final URL-level dedupe while preserving order\n",
        "    seen_final: set[str] = set()\n",
        "    final_rows: list[dict[str, str]] = []\n",
        "    for r in merged:\n",
        "        u = r[\"url\"]\n",
        "        if u and u not in seen_final:\n",
        "            seen_final.add(u)\n",
        "            final_rows.append(r)\n",
        "\n",
        "    _write_seed_csv(out_path, final_rows)\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "GGUgqsn4nZUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl for Dataset"
      ],
      "metadata": {
        "id": "DNhpdB2gnhJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"Free music download websites in SA\",\n",
        "    \"Amapiano Free Music download\",\n",
        "    \"south africa mp3 download\",\n",
        "    \"royalty free african music download\",\n",
        "    \"South African hip hop free download\",\n",
        "    \"Free Kwaito music downloads\",\n",
        "    \"Download South African house music\",\n",
        "    \"Free Gqom tracks download South Africa\",\n",
        "    \"Download African jazz mp3\",\n",
        "    \"Free maskandi music download\",\n",
        "    \"Free South African gospel download mp3\",\n",
        "    \"Free mp3 download site:co.za\",\n",
        "    \"Free music download archives South Africa\",\n",
        "    \"Download latest SA music free\",\n",
        "    \"Free African pop music download\",\n",
        "    \"Free traditional South African music download\",\n",
        "    \"Free mp3 download no registration South Africa\",\n",
        "    \"Free music download legal South Africa\",\n",
        "    \"Free Afrobeat music download\",\n",
        "    \"Free music download South Africa\",\n",
        "]\n",
        "\n",
        "async def crawl_dataset():\n",
        "    path = await seed_urls_from_queries(\n",
        "        queries=queries,\n",
        "        out_csv=\"../data/crawler_dataset.csv\",\n",
        "        min_results_per_query=60,\n",
        "        results_per_page=20,\n",
        "        headless=False,\n",
        "        screenshots=False,\n",
        "        dedupe_by_host=True,\n",
        "        max_per_host=2,\n",
        "        default_label=\"\",\n",
        "    )\n",
        "    print(f\"Wrote seeds to: {path.resolve()}\")\n",
        "\n",
        "asyncio.run(crawl_dataset())"
      ],
      "metadata": {
        "id": "hOlbjIayno58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "8WwZ5yybmqPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "MODEL_PATH = BASE_DIR / \"models\"\n",
        "DATA_DIR = (BASE_DIR / \"data\")\n",
        "PAGES_DIR = (DATA_DIR / \"pages\")\n",
        "\n",
        "LABELS_CSV = DATA_DIR / \"labels.csv\" # url,label where label ∈ {'pirated','legit'}\n",
        "MODEL_FILE = MODEL_PATH / \"music_piracy_classifier.joblib\"\n",
        "\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PAGES_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "KMh7k4MBmtm_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4217a0"
      },
      "source": [
        "# Load and preprocess labels data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lI5KoE5es3El",
        "outputId": "5825fa58-e557-4d09-d724-bffe6fdb7e8d"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "pd.set_option('display.max_colwidth', 120)\n",
        "\n",
        "if not LABELS_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Labels csv file not found at {LABELS_CSV}\")\n",
        "\n",
        "unbalanced_labels = pd.read_csv(LABELS_CSV).dropna()\n",
        "min_count = unbalanced_labels['label'].value_counts().min()\n",
        "labels = (\n",
        "    unbalanced_labels.groupby('label', group_keys=False)\n",
        "    .apply(lambda x: x.sample(min_count, random_state=RANDOM_SEED))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "labels['label'] = labels['label'].str.strip().str.lower()\n",
        "labels = labels[labels['label'].isin(['pirated','legit'])].drop_duplicates('url')\n",
        "print(labels.head(8))\n",
        "print(labels['label'].value_counts())"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Labels csv file not found at /content/data/labels.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4220487566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLABELS_CSV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Labels csv file not found at {LABELS_CSV}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0munbalanced_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABELS_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Labels csv file not found at /content/data/labels.csv"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c56d8781"
      },
      "source": [
        "# Add url data and url features to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P88EevdztMrd"
      },
      "source": [
        "from utilities import url_features, fetch_page_data\n",
        "\n",
        "def build_dataset(df: pd.DataFrame, force: bool=False) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        url = row['url']\n",
        "        label = row['label']\n",
        "\n",
        "        page = fetch_page_data(url, force=force)\n",
        "        text = page.text if page.text else \"\"\n",
        "        title, desc = page.title, page.desc\n",
        "\n",
        "        keywords = \" \".join([title, desc, text])[:150000]\n",
        "\n",
        "        rows.append({\n",
        "            \"url\": url,\n",
        "            \"label\": label,\n",
        "            \"title\": title,\n",
        "            \"desc\": desc,\n",
        "            \"text\": text,\n",
        "            **url_features(url),\n",
        "        })\n",
        "    return pd.DataFrame(rows).reset_index(drop=True)\n",
        "\n",
        "data = build_dataset(labels, force=False)\n",
        "print(data.head(5))\n",
        "data.to_csv(\"../data/dataset.csv\", index=False)\n",
        "numeric_cols = ['url_length','num_digits','num_hyphens','num_underscores','num_params','num_slashes','scheme_https','subdomain_len','num_subdomain_parts','tld_len','path_len']\n",
        "print(data[numeric_cols].describe().T)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQeLR8g_lsrc"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ec4af24"
      },
      "source": [
        "# Split data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syAboGJKuQiQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine text columns into a single text column for feature extraction\n",
        "data[\"text_data\"] = (\n",
        "    data[\"title\"].fillna(\"\") + \" \" +\n",
        "    data[\"desc\"].fillna(\"\") + \" \" +\n",
        "    data[\"text\"].fillna(\"\")\n",
        ")\n",
        "x = data[['url',\"text_data\"] + numeric_cols].copy()\n",
        "y = data['label'].map({'pirated':1, 'legit':0})  # 1 = pirated\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if y.nunique() > 1 else None\n",
        ")\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.mean(), y_test.mean())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcbed1d2"
      },
      "source": [
        "# Setup Machine Learning Pipeline with Feature Extraction and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCtIYjPKugim"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Baseline: URL char n-grams + page text word n-grams + simple numeric features\n",
        "url_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, max_features=20000)\n",
        "text_vectorizer = TfidfVectorizer(lowercase=True,ngram_range=(1,2), min_df=2, max_features=50000)\n",
        "\n",
        "def get_numeric(x):\n",
        "    return x[numeric_cols]\n",
        "\n",
        "numeric_transformer = FunctionTransformer(get_numeric, validate=False)\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('url_tfidf', url_vectorizer, 'url'),\n",
        "        ('text_tfidf', text_vectorizer, 'text_data'),\n",
        "        ('num', 'passthrough', numeric_cols)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=5000, class_weight='balanced', n_jobs=None)\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('clf', clf)\n",
        "])\n",
        "\n",
        "pipe"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ecb876"
      },
      "source": [
        "# Train the Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1lgZb9qvT52"
      },
      "source": [
        "\n",
        "pipe.fit(x_train, y_train)\n",
        "print(\"Trained.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e1817f8"
      },
      "source": [
        "# Evaluate the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH8H_amUvXEf"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "# rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "# Classification Report\n",
        "y_pred = pipe.predict(x_test)\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['legit', 'pirated'])\n",
        "fig, ax = plt.subplots()\n",
        "disp.plot(ax=ax, xticks_rotation=45)\n",
        "for text in ax.texts:\n",
        "    text.set_color(\"white\")\n",
        "ax.set_title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Probabilities for ROC (LogReg has predict_proba)\n",
        "try:\n",
        "    y_prob = pipe.predict_proba(x_test)[:,1]\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"ROC AUC: {auc:.3f}\")\n",
        "    RocCurveDisplay.from_predictions(y_test, y_prob)\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Could not compute probabilities/ROC:\", e)\n",
        "\n",
        "# Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) if y_train.nunique() > 1 else None\n",
        "if skf:\n",
        "    scores = cross_val_score(pipe, x_train, y_train, cv=skf, scoring='f1')\n",
        "    print(\"CV F1 (mean ± std):\", np.mean(scores).round(3), \"±\", np.std(scores).round(3))\n",
        "else:\n",
        "    print(\"Not enough class variety for CV.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96b61815"
      },
      "source": [
        "# Hyperparameter Tuning with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_zPQU1vdhb"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'preprocess__url_tfidf__ngram_range': [(3,5), (3,6)],\n",
        "    'preprocess__text_tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'clf__C': [0.5, 1.0, 2.0]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, scoring='f1', cv=3, n_jobs=-1, verbose=1)\n",
        "gs.fit(x_train, y_train)\n",
        "print(\"Best params:\", gs.best_params_)\n",
        "print(\"Best CV F1:\", gs.best_score_)\n",
        "\n",
        "best_model = gs.best_estimator_\n",
        "y_pred_gs = best_model.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_gs, digits=3))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf997f97"
      },
      "source": [
        "# Save the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5G3flq4vg4B"
      },
      "source": [
        "import joblib\n",
        "from config import MODEL_FILE\n",
        "\n",
        "final_model = best_model if 'best_model' in locals() else pipe\n",
        "joblib.dump(final_model, MODEL_FILE)\n",
        "print(f\"Saved: {MODEL_FILE}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75610114"
      },
      "source": [
        "# Classify a New URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQQ4YA-PvjHs"
      },
      "source": [
        "def classify_url(url: str, model=None) -> dict:\n",
        "    model = model or joblib.load(MODEL_FILE)\n",
        "    page = fetch_page_data(url, force=False)\n",
        "    row = {\n",
        "        \"url\": url,\n",
        "        \"title\": page.title,\n",
        "        \"desc\": page.desc,\n",
        "        \"text\": page.text,\n",
        "        **url_features(url),\n",
        "    }\n",
        "    row[\"text_data\"] = (row[\"title\"] or \"\") + \" \" + (row[\"desc\"] or \"\") + \" \" + (row[\"text\"] or \"\")\n",
        "    df = pd.DataFrame([row])\n",
        "    pred = model.predict(df)[0]\n",
        "    out = {\"url\": url, \"pred_label\": \"pirated\" if int(pred)==1 else \"legit\"}\n",
        "    try:\n",
        "        proba = model.predict_proba(df)[0,1]\n",
        "        out[\"pirated_probability\"] = float(proba)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "print(classify_url(\"https://fakaza.com/\"))\n",
        "print(classify_url(\"https://mzanzitunes.com/\"))\n",
        "print(classify_url(\"https://music.apple.com/\"))\n",
        "print(classify_url(\"https://spotify.com/\"))\n",
        "print(classify_url(\"https://tubidy.com/\"))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fW063Pn0r1-k",
        "0Vq9B_Uzq_Aw",
        "2wT_OpPH3I5c",
        "uLydGW_Vkz0W",
        "ySSh5MnQsvn4",
        "c56d8781",
        "6ec4af24",
        "bcbed1d2",
        "58ecb876",
        "3e1817f8",
        "96b61815",
        "cf997f97",
        "75610114"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}